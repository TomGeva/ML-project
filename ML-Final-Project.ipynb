{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.csv & test.csv\n",
    "- **price_doc**: sale price (this is the target variable)\n",
    "- **id**: transaction id\n",
    "- **timestamp**: date of transaction\n",
    "- **full_sq**: total area in square meters, including loggias, balconies and other non-residential areas\n",
    "- **life_sq**: living area in square meters, excluding loggias, balconies and other non-residential areas\n",
    "- **floor**: for apartments, floor of the building\n",
    "- **max_floor**: number of floors in the building\n",
    "- **material**: wall material\n",
    "- **build_year**: year built\n",
    "- **num_room**: number of living rooms\n",
    "- **kitch_sq**: kitchen area\n",
    "- **state**: apartment condition\n",
    "- **product_type**: owner-occupier purchase or investment\n",
    "- **sub_area**: name of the district\n",
    "\n",
    "The dataset also includes a collection of features about each property's surrounding neighbourhood, and some features that are constant across each sub area (known as a Raion). Most of the feature names are self explanatory, with the following notes. See below for a complete list.\n",
    "- **full_all**: subarea population\n",
    "- **male_f**, **female_f**: subarea population by gender\n",
    "- **young_***: population younger than working age\n",
    "- **work_***: working-age population\n",
    "- **ekder_***: retirement-age population\n",
    "- **n_m_{all|male|female}**: population between n and m years old\n",
    "- **build_count_***: buildings in the subarea by construction type or year\n",
    "- **x_count_500**: the number of x within 500m of the property\n",
    "- **x_part_500**: the share of x within 500m of the property\n",
    "- **\\_sqm_**: square meters\n",
    "- **cafe_count_d_price_p**: number of cafes within d meters of the property that have an average bill under p RUB\n",
    "- **trc_**: shopping malls\n",
    "- **prom_**: industrial zones\n",
    "- **green_**: green zones\n",
    "- **metro_**: subway\n",
    "- **\\_avto_**: distances by car\n",
    "- **mkad_**: Moscow Circle Auto Road\n",
    "- **ttk_**: Third Transport Ring\n",
    "- **sadovoe_**: Garden Ring\n",
    "- **bulvar_ring_**: Boulevard Ring\n",
    "- **kremlin_**: City center\n",
    "- **zd_vokzaly_**: Train station\n",
    "- **oil_chemistry_**: Dirty industry\n",
    "- **ts_**: Power plant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### macro.csv\n",
    "A set of macroeconomic indicators, one for each date.\n",
    "\n",
    "- **timestamp**: Transaction timestamp\n",
    "- **oil_urals**: Crude Oil Urals ($/bbl)\n",
    "- **gdp_quart**: GDP\n",
    "- **gdp_quart_growth**: Real GDP growth\n",
    "- **cpi**: Inflation - Consumer Price Index Growth\n",
    "- **ppi**: Inflation - Producer Price index Growth\n",
    "- **gdp_deflator**: Inflation - GDP deflator\n",
    "- **balance_trade**: Trade surplus\n",
    "- **balance_trade_growth**: Trade balance (as a percentage of previous year)\n",
    "- **usdrub**: Ruble/USD exchange rate\n",
    "- **eurrub**: Ruble/EUR exchange rate\n",
    "- **brent**: London Brent ($/bbl)\n",
    "- **net_capital_export**: Net import / export of capital\n",
    "- **gdp_annual**: GDP at current prices\n",
    "- **gdp_annual_growth**: GDP growth (in real terms)\n",
    "- **average_provision_of_build_contract**: Provision by orders in Russia (for the developer)\n",
    "- **average_provision_of_build_contract_moscow**: Provision by orders in Moscow (for the developer)\n",
    "- **rts**: Index RTS / return\n",
    "- **micex**: MICEX index / return\n",
    "- **micex_rgbi_tr**: MICEX index for government bonds (MICEX RGBI TR) / yield\n",
    "- **micex_cbi_tr**: MICEX Index corporate bonds (MICEX CBI TR) / yield\n",
    "- **deposits_value**: Volume of household deposits\n",
    "- **deposits_growth**: Volume growth of population's deposits\n",
    "- **deposits_rate**: Average interest rate on deposits\n",
    "- **mortgage_value**: Volume of mortgage loans\n",
    "- **mortgage_growth**: Growth of mortgage lending\n",
    "- **mortgage_rate**: Weighted average rate of mortgage loans\n",
    "- **grp**: GRP of the subject of Russian Federation where Apartment is located\n",
    "- **grp_growth**: Growth of gross regional product of the subject of the Russian Federation where Apartment is located\n",
    "- **income_per_cap**: Average income per capita \n",
    "- **real_dispos_income_per_cap_growth**: Growth in real disposable income of Population\n",
    "- **salary**: Average monthly salary\n",
    "- **salary_growth**: Growth of nominal wages\n",
    "- **fixed_basket**: Cost of a fixed basket of consumer goods and services for inter-regional comparisons of purchasing power\n",
    "- **retail_trade_turnover**: Retail trade turnover\n",
    "- **retail_trade_turnover_per_cap**: Retail trade turnover per capita\n",
    "- **retail_trade_turnover_growth**: Retail turnover (in comparable prices in% to corresponding period of previous year)\n",
    "- **labor_force**: Size of labor force\n",
    "- **unemployment**: Unemployment rate\n",
    "- **employment**: Employment rate\n",
    "- **invest_fixed_capital_per_cap**: Investments in fixed capital per capita\n",
    "- **invest_fixed_assets**: Absolute volume of investments in fixed assets\n",
    "- **profitable_enterpr_share**: Share of profitable enterprises\n",
    "- **unprofitable_enterpr_share**: The share of unprofitable enterprises\n",
    "- **share_own_revenues**: The share of own revenues in the total consolidated budget revenues\n",
    "- **overdue_wages_per_cap**: Overdue wages per person\n",
    "- **fin_res_per_cap**: The financial results of companies per capita\n",
    "- **marriages_per_1000_cap**: Number of marriages per 1,000 people\n",
    "- **divorce_rate**: The divorce rate / growth rate\n",
    "- **construction_value**: Volume of construction work performed (million rubles)\n",
    "- **invest_fixed_assets_phys**: The index of physical volume of investment in fixed assets (in comparable prices in% to the corresponding month of Previous year)\n",
    "- **pop_natural_increase**: Rate of natural increase / decrease in Population (1,000 persons)\n",
    "- **pop_migration**: Migration increase (decrease) of population\n",
    "- **pop_total_inc**: Total population growth\n",
    "- **childbirth**: Childbirth\n",
    "- **mortality**: Mortality\n",
    "- **housing_fund_sqm**: Housing Fund (sqm)\n",
    "- **lodging_sqm_per_cap**: Lodging (sqm / pax)\n",
    "- **water_pipes_share**: Plumbing availability (pax)\n",
    "- **baths_share**: Bath availability (pax)\n",
    "- **sewerage_share**: Canalization availability\n",
    "- **gas_share**: Gas (mains, liquefied) availability\n",
    "- **hot_water_share**: Hot water availability\n",
    "- **electric_stove_share**: Electric heating for the floor\n",
    "- **heating_share**: Heating availability\n",
    "- **old_house_share**: Proportion of old and dilapidated housing, percent\n",
    "- **average_life_exp**: Average life expectancy\n",
    "- **infant_mortarity_per_1000_cap**: Infant mortality rate (per 1,000 children aged up to one year)\n",
    "- **perinatal_mort_per_1000_cap**: Perinatal mortality rate (per 1,000 live births)\n",
    "- **incidence_population**: Overall incidence of the total population\n",
    "- **rent_price_4+room_bus**: rent price for 4-room apartment, business class\n",
    "- **rent_price_3room_bus**: rent price for 3-room apartment, business class\n",
    "- **rent_price_2room_bus**: rent price for 2-room apartment, business class\n",
    "- **rent_price_1room_bus**: rent price for 1-room apartment, business class\n",
    "- **rent_price_3room_eco**: rent price for 3-room apartment, econom class\n",
    "- **rent_price_2room_eco**: rent price for 2-room apartment, econom class\n",
    "- **rent_price_1room_eco**: rent price for 1-room apartment, econom class\n",
    "- **load_of_teachers_preschool_per_teacher**: Load of teachers of preschool educational institutions (number of children per 100 teachers);\n",
    "- **child_on_acc_pre_school**: Number of children waiting for the determination to pre-school educational institutions, for capacity of 100\n",
    "- **load_of_teachers_school_per_teacher**: Load on teachers in high school (number of pupils in hugh school for 100 teachers)\n",
    "- **students_state_oneshift**: Proportion of pupils in high schools with one shift, of the total number of pupils in high schools\n",
    "- **modern_education_share**: Share of state (municipal) educational organizations, corresponding to modern requirements of education in the total number of high schools;\n",
    "- **old_education_build_share**: The share of state (municipal) educational organizations, buildings are in disrepair and in need of major repairs of the total number.\n",
    "- **provision_doctors**: Provision (relative number) of medical doctors in area\n",
    "- **provision_nurse**: Provision of nursing staff\n",
    "- **load_on_doctors**: The load on doctors (number of visits per physician)\n",
    "- **power_clinics**: Capacity of outpatient clinics\n",
    "- **hospital_beds_available_per_cap**: Availability of hospital beds per 100 000 persons\n",
    "- **hospital_bed_occupancy_per_year**: Average occupancy rate of the hospital beds during a year\n",
    "- **provision_retail_space_sqm**: Retail space\n",
    "- **provision_retail_space_modern_sqm**: Provision of population with retail space of modern formats, square meter\n",
    "- **retail_trade_turnover_per_cap**: Retail trade turnover per capita\n",
    "- **turnover_catering_per_cap**: Turnover of catering industry per person\n",
    "- **theaters_viewers_per_1000_cap**: Number of theaters viewers per 1000 population\n",
    "- **seats_theather_rfmin_per_100000_cap**: Total number of seats in Auditorium of the Ministry of Culture Russian theaters per 100,000 population\n",
    "- **museum_visitis_per_100_cap**: Number of visits to museums per 1000 of population\n",
    "- **bandwidth_sports**: Capacity of sports facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grdspc\n",
    "import seaborn as sns\n",
    "import random as rnd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR, GradientBoostingRegressor as GBR\n",
    "from sklearn.model_selection import GridSearchCV as GSCV, cross_val_score as CVS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline Model Development\n",
    "## a. Baseline Model (from Task 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from train.csv & test.csv\n",
    "##### For original model from task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SbrBkTrain = pd.read_csv('train.csv', parse_dates=['timestamp']).filter(['full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'kitch_sq', 'state', 'product_type', 'sub_area', 'price_doc', 'timestamp'])\n",
    "SbrBkTest = pd.read_csv('test.csv', parse_dates=['timestamp']).filter(['id', 'full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'kitch_sq', 'state', 'product_type', 'sub_area', 'timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data cleaning\n",
    "# full_sq should be a value between 7 and 500 from my observation\n",
    "SbrBkTrain.full_sq.loc[SbrBkTrain.full_sq < 7] = np.nan\n",
    "SbrBkTrain.full_sq.loc[SbrBkTrain.full_sq > 500] = np.nan\n",
    "# life_sq should be a value between 7 and 300 from my observation\n",
    "SbrBkTrain.life_sq.loc[SbrBkTrain.life_sq < 7] = np.nan\n",
    "SbrBkTrain.life_sq.loc[SbrBkTrain.life_sq > 300] = np.nan\n",
    "# floor should not be a value above 40 from my observation\n",
    "SbrBkTrain.floor.loc[SbrBkTrain.floor > 40] = np.nan\n",
    "# max_floor should not be a value above 50 from my observation\n",
    "SbrBkTrain.max_floor.loc[SbrBkTrain.max_floor > 50] = np.nan\n",
    "# max_floor should not be a value below the value of floor in the same row\n",
    "SbrBkTrain.max_floor.loc[SbrBkTrain.max_floor < SbrBkTrain.floor] = np.nan\n",
    "# build_year should not be before 1800 and above 2020 from my observation\n",
    "SbrBkTrain.build_year.loc[SbrBkTrain.build_year < 1800] = np.nan \n",
    "SbrBkTrain.build_year.loc[SbrBkTrain.build_year > 2020] = np.nan\n",
    "# num_room should be a value between 1 and 12 from my observation\n",
    "SbrBkTrain.num_room.loc[SbrBkTrain.num_room < 1] = np.nan\n",
    "SbrBkTrain.num_room.loc[SbrBkTrain.num_room > 12] = np.nan\n",
    "# kitch_sq should not be a value below 2 and above 250 from my observation\n",
    "SbrBkTrain.kitch_sq.loc[SbrBkTrain.kitch_sq < 2] = np.nan\n",
    "SbrBkTrain.kitch_sq.loc[SbrBkTrain.kitch_sq > 250] = np.nan\n",
    "# full_sq should not be a value below life_sq in the same row\n",
    "SbrBkTrain.life_sq.loc[SbrBkTrain.life_sq > SbrBkTrain.full_sq] = np.nan\n",
    "# full_sq should not be a value below kitch_sq in the same row\n",
    "SbrBkTrain.kitch_sq.loc[SbrBkTrain.full_sq < SbrBkTrain.kitch_sq] = np.nan\n",
    "# kitch_sq part of full_sq should not be above life_sq part of full_sq in the same row, as long as full_sq is greater than the sum of life_sq and kitch_sq\n",
    "SbrBkTrain.kitch_sq.loc[(SbrBkTrain.kitch_sq / SbrBkTrain.full_sq > SbrBkTrain.life_sq / SbrBkTrain.full_sq) & (SbrBkTrain.full_sq >= SbrBkTrain.life_sq + SbrBkTrain.kitch_sq)] = np.nan\n",
    "# full_sq should not be a value below the sum of life_sq and kitch_sq in the same row\n",
    "SbrBkTrain.life_sq.loc[SbrBkTrain.full_sq < SbrBkTrain.life_sq + SbrBkTrain.kitch_sq] = SbrBkTrain.full_sq[SbrBkTrain.full_sq < SbrBkTrain.life_sq + SbrBkTrain.kitch_sq] - SbrBkTrain.kitch_sq.loc[SbrBkTrain.full_sq < SbrBkTrain.life_sq + SbrBkTrain.kitch_sq]\n",
    "# fixing the state values that are incorrect due to misstyping\n",
    "SbrBkTrain.state.loc[SbrBkTrain.state > 4] = SbrBkTrain.state.loc[SbrBkTrain.state > 4] // 10\n",
    "\n",
    "# Test data cleaning\n",
    "# full_sq should be a value between 7 and 500 from my observation\n",
    "SbrBkTest.full_sq.loc[SbrBkTest.full_sq < 7] = np.nan\n",
    "SbrBkTest.full_sq.loc[SbrBkTest.full_sq > 500] = np.nan\n",
    "# life_sq should be a value between 7 and 300 from my observation\n",
    "SbrBkTest.life_sq.loc[SbrBkTest.life_sq < 7] = np.nan\n",
    "SbrBkTest.life_sq.loc[SbrBkTest.life_sq > 300] = np.nan\n",
    "# floor should not be a value above 40 from my observation\n",
    "SbrBkTest.floor.loc[SbrBkTest.floor > 40] = np.nan\n",
    "# max_floor should not be a value above 50 from my observation\n",
    "SbrBkTest.max_floor.loc[SbrBkTest.max_floor > 50] = np.nan\n",
    "# max_floor should not be a value below the value of floor in the same row\n",
    "SbrBkTest.max_floor.loc[SbrBkTest.max_floor < SbrBkTest.floor] = np.nan\n",
    "# build_year should not be before 1800 and above 2020 from my observation\n",
    "SbrBkTest.build_year.loc[SbrBkTest.build_year < 1800] = np.nan \n",
    "SbrBkTest.build_year.loc[SbrBkTest.build_year > 2020] = np.nan\n",
    "# num_room should be a value between 1 and 12 from my observation\n",
    "SbrBkTest.num_room.loc[SbrBkTest.num_room < 1] = np.nan\n",
    "SbrBkTest.num_room.loc[SbrBkTest.num_room > 12] = np.nan\n",
    "# kitch_sq should not be a value below 2 and above 250 from my observation\n",
    "# SbrBkTest.kitch_sq[SbrBkTest.kitch_sq < 2] = np.nan\n",
    "# SbrBkTest.kitch_sq[SbrBkTest.kitch_sq > 250] = np.nan\n",
    "# full_sq should not be a value below life_sq in the same row\n",
    "SbrBkTest.life_sq.loc[SbrBkTest.life_sq > SbrBkTest.full_sq] = np.nan\n",
    "# full_sq should not be a value below kitch_sq in the same row\n",
    "SbrBkTest.kitch_sq.loc[SbrBkTest.full_sq < SbrBkTest.kitch_sq] = np.nan\n",
    "# kitch_sq part of full_sq should not be above life_sq part of full_sq in the same row, as long as full_sq is greater than the sum of life_sq and kitch_sq\n",
    "SbrBkTest.kitch_sq.loc[(SbrBkTest.kitch_sq / SbrBkTest.full_sq > SbrBkTest.life_sq / SbrBkTest.full_sq) & (SbrBkTest.full_sq >= SbrBkTest.life_sq + SbrBkTest.kitch_sq)] = np.nan\n",
    "# full_sq should not be a value below the sum of life_sq and kitch_sq in the same row\n",
    "SbrBkTest.life_sq.loc[SbrBkTest.full_sq < SbrBkTest.life_sq + SbrBkTest.kitch_sq] = SbrBkTest.full_sq.loc[SbrBkTest.full_sq < SbrBkTest.life_sq + SbrBkTest.kitch_sq] - SbrBkTest.kitch_sq[SbrBkTest.full_sq < SbrBkTest.life_sq + SbrBkTest.kitch_sq]\n",
    "# fixing the state values that are incorrect due to misstyping\n",
    "SbrBkTest.state.loc[SbrBkTest.state > 4] = SbrBkTest.state.loc[SbrBkTest.state > 4] // 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Handling of missing values\n",
    "\n",
    "##### Viewing abnormalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precentages of missing values in Train:\\n{(SbrBkTrain.isna().sum() / SbrBkTrain.shape[0] * 100).round(2).sort_values(ascending = False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### handling abnormalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SbrBkTrain.drop(columns = ['kitch_sq'], inplace = True)\n",
    "SbrBkTest.drop(columns = ['kitch_sq'], inplace = True)\n",
    "\n",
    "SbrBkTrain['toDrop'] = (((SbrBkTrain.build_year.isna()) | (SbrBkTrain.state.isna())) & (SbrBkTrain.isna().sum(axis = 1) > 5))\n",
    "# marking rows that are randomly selected to be dropped\n",
    "SbrBkTrain.toDrop[rnd.sample(list((((SbrBkTrain.build_year.isna()) | (SbrBkTrain.state.isna())) & (SbrBkTrain.isna().sum(axis = 1) == 5))[(((SbrBkTrain.build_year.isna()) | (SbrBkTrain.state.isna())) & (SbrBkTrain.isna().sum(axis = 1) == 5))==True].index), 3300)] = True\n",
    "# dropping all marked rows\n",
    "SbrBkTrain = SbrBkTrain[~SbrBkTrain.toDrop]\n",
    "SbrBkTrain.drop(columns = ['toDrop'], inplace = True)\n",
    "\n",
    "# viewing the precentages again\n",
    "print(f'Precentages of missing values in Train:\\n{(SbrBkTrain.isna().sum() / SbrBkTrain.shape[0] * 100).round(2).sort_values(ascending = False)}')\n",
    "print(f'Precentages of missing values in Test:\\n{(SbrBkTest.isna().sum() / SbrBkTest.shape[0] * 100).round(2).sort_values(ascending = False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SbrBkTrain['product_typeCode'] = pd.Categorical(SbrBkTrain.product_type).codes\n",
    "SbrBkTest['product_typeCode'] = pd.Categorical(SbrBkTest.product_type).codes\n",
    "SbrBkTest.product_typeCode[SbrBkTest.product_typeCode == -1] = np.nan\n",
    "SbrBkTrain['sub_areaCode'] = pd.Categorical(SbrBkTrain.sub_area).codes\n",
    "SbrBkTest['sub_areaCode'] = pd.Categorical(SbrBkTest.sub_area).codes\n",
    "\n",
    "imputedTrainData = KNNImputer(n_neighbors = 20).fit_transform(SbrBkTrain[['full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'state', 'product_typeCode', 'sub_areaCode']])\n",
    "imputedTestData = KNNImputer(n_neighbors = 20).fit_transform(SbrBkTest[['full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'state', 'product_typeCode', 'sub_areaCode']])\n",
    "imputedTrain = pd.DataFrame(imputedTrainData, columns = ['full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'state', 'product_typeCode', 'sub_areaCode'])\n",
    "imputedTest = pd.DataFrame(imputedTestData, columns = ['full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'state', 'product_typeCode', 'sub_areaCode'])\n",
    "# as observed previously all of the features are integers, and they should remain as such\n",
    "imputedTrain = imputedTrain.round()\n",
    "imputedTest = imputedTest.round()\n",
    "imputedTrain = imputedTrain.set_index(SbrBkTrain.index)\n",
    "imputedTest = imputedTest.set_index(SbrBkTest.index)\n",
    "imputedTrain['product_type'] = SbrBkTrain.product_type\n",
    "imputedTest['product_type'] = SbrBkTest.product_type\n",
    "imputedTrain['sub_area'] = SbrBkTrain.sub_area\n",
    "imputedTest['sub_area'] = SbrBkTest.sub_area\n",
    "imputedTrain['price_doc'] = SbrBkTrain.price_doc\n",
    "imputedTest['id'] = SbrBkTest.id\n",
    "imputedTrain['timestamp'] = SbrBkTrain.timestamp\n",
    "imputedTest['timestamp'] = SbrBkTest.timestamp\n",
    "imputedTrain.product_type.loc[imputedTrain.product_type.isna()] = np.where(imputedTrain.product_typeCode[imputedTrain.product_type.isna()] == 0, 'Investment', 'OwnerOccupier')\n",
    "imputedTest.product_type.loc[imputedTest.product_type.isna()] = np.where(imputedTest.product_typeCode[imputedTest.product_type.isna()] == 0, 'Investment', 'OwnerOccupier')\n",
    "imputedTrain.drop(columns = ['product_typeCode', 'sub_areaCode'], inplace = True)\n",
    "imputedTest.drop(columns = ['product_typeCode', 'sub_areaCode'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputedTrain = imputedTrain.assign(roomPerSq = imputedTrain.num_room / imputedTrain.full_sq,\n",
    "                                   buildingAge = imputedTrain.timestamp.dt.year - imputedTrain.build_year,\n",
    "                                   practAging = imputedTrain.buildingAge / imputedTrain.state,\n",
    "                                   roomSize = imputedTrain.life_sq / imputedTrain.num_room,\n",
    "                                   heightScore = (imputedTrain.floor - imputedTrain.max_floor * 0.37).abs() / (imputedTrain.max_floor * 0.63 / 10),\n",
    "                                   lifeRatio = imputedTrain.life_sq / (imputedTrain.full_sq / 10),\n",
    "                                   month = imputedTrain.timestamp.dt.month)\n",
    "imputedTest = imputedTest.assign(roomPerSq = imputedTest.num_room / imputedTest.full_sq,\n",
    "                                 buildingAge = imputedTest.timestamp.dt.year - imputedTest.build_year,\n",
    "                                 practAging = imputedTest.buildingAge / imputedTest.state,\n",
    "                                 roomSize = imputedTest.life_sq / imputedTest.num_room,\n",
    "                                 heightScore = (imputedTest.floor - imputedTest.max_floor * 0.37).abs() / (imputedTest.max_floor * 0.63 / 10),\n",
    "                                 lifeRatio = imputedTest.life_sq / (imputedTest.full_sq / 10),\n",
    "                                 month = imputedTest.timestamp.dt.month)\n",
    "imputedTrain.heightScore[imputedTrain.heightScore.isna()] = 7\n",
    "imputedTest.heightScore[imputedTest.heightScore.isna()] = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twicked from Original Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in SbrBkTrain.columns:\n",
    "    if SbrBkTrain[col].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(SbrBkTrain[col].values) + list(SbrBkTest[col].values))\n",
    "        SbrBkTrain[col] = lbl.transform(list(SbrBkTrain[col].values))\n",
    "        SbrBkTest[col] = lbl.transform(list(SbrBkTest[col].values))\n",
    "\n",
    "# addition none object labeling\n",
    "mtrlLbl = LabelEncoder().fit(list(SbrBkTrain.material.values) + list(SbrBkTest.material.values))\n",
    "stateLbl = LabelEncoder().fit(list(SbrBkTrain.state.values) + list(SbrBkTest.state.values))\n",
    "SbrBkTrain.material = mtrlLbl.transform(SbrBkTrain.material.values)\n",
    "SbrBkTest.material = mtrlLbl.transform(SbrBkTest.material.values)\n",
    "SbrBkTrain.state = stateLbl.transform(SbrBkTrain.state.values)\n",
    "SbrBkTest.state = stateLbl.transform(SbrBkTest.state.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling full_sq in imputedTrain and impuedTest\n",
    "scaledMaxFullSq = imputedUnion.full_sq.max() / 100\n",
    "imputedTrain.full_sq = imputedTrain.full_sq / scaledMaxFullSq\n",
    "imputedTest.full_sq = imputedTest.full_sq / scaledMaxFullSq\n",
    "\n",
    "# Rescaling life_sq in imputedTrain and impuedTest\n",
    "scaledMaxLifeSq = imputedUnion.life_sq.max() / 100\n",
    "imputedTrain.life_sq = imputedTrain.life_sq / scaledMaxLifeSq\n",
    "imputedTest.life_sq = imputedTest.life_sq / scaledMaxLifeSq\n",
    "\n",
    "# Rescaling build_year in imputedTrain and impuedTest\n",
    "minBuildYear = imputedUnion.build_year.min()\n",
    "imputedTrain.build_year = imputedTrain.build_year - minBuildYear\n",
    "imputedTest.build_year = imputedTest.build_year - minBuildYear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Train/Validation/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTrain = imputedTrain.copy(deep = True).reset_index(drop = True)\n",
    "finalTest = imputedTest.copy(deep = True).reset_index(drop = True)\n",
    "\n",
    "# due to it being a time series it is only appropriate to split the data in a chronological manner\n",
    "TrainData = finalTrain.loc[:int(finalTrain.shape[0] * 0.7), :]\n",
    "xTrn = TrainData.drop(columns = ['price_doc', 'logPrice', 'timestamp'])\n",
    "yTrn = TrainData.logPrice\n",
    "ValidationData = finalTrain.loc[int(finalTrain.shape[0] * 0.7): int(finalTrain.shape[0] * 0.85), :]\n",
    "xVal = ValidationData.drop(columns = ['price_doc', 'logPrice', 'timestamp'])\n",
    "yVal = ValidationData.logPrice\n",
    "TestData = finalTrain.loc[int(finalTrain.shape[0] * 0.85):, :]\n",
    "xTst = TestData.drop(columns = ['price_doc', 'logPrice', 'timestamp'])\n",
    "yTst = TestData.logPrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftrSlctModel = RFR(n_jobs = -1, random_state = 42)\n",
    "ftrSlctModel.fit(xTrn, yTrn)\n",
    "imprtnc = ftrSlctModel.feature_importances_\n",
    "features = pd.Series(imprtnc, index = xTrn.columns).sort_values(ascending = False)\n",
    "features = list(features[features > 0.05].index)\n",
    "\n",
    "rndFrstBaseModel = RFR()\n",
    "rndFrstParams = {\n",
    "    'max_depth': [i for i in range(4, 9, 2)],\n",
    "    'n_estimators': [i for i in range(100, 301, 50)],\n",
    "    'min_samples_leaf': [i for i in range(3, 10, 3)]\n",
    "}\n",
    "\n",
    "grdSrcRndFrst = GSCV(rndFrstBaseModel, rndFrstParams, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "grdSrcRndFrst.fit(xTrn[features], yTrn)\n",
    "\n",
    "RndFrstModelParams = grdSrcRndFrst.best_params_\n",
    "\n",
    "rndFrstOptModel = RFR(**RndFrstModelParams)\n",
    "rndFrstOptModel.fit(xTrn[features], yTrn)\n",
    "\n",
    "rndFrstMseValScores = -1 * CVS(rndFrstOptModel, xVal[features], yVal, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "rndFrstMseTstScores = -1 * CVS(rndFrstOptModel, xTst[features], yTst, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "print(f'The mean squared error of the model on the validation set is {rndFrstMseValScores.mean()}')\n",
    "print(f'The mean squared error of the model on the Test set is {rndFrstMseTstScores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbBaseModel = GBR()\n",
    "\n",
    "xgbParams = {\n",
    "    'learning_rate': [i/1000 for i in range(6, 15, 4)],\n",
    "    'n_estimators': [i for i in range(100, 301, 100)],\n",
    "    'subsample': [i/20 for i in range(13, 18, 2)],\n",
    "    'max_depth': [i for i in range(4, 9, 2)],\n",
    "    'min_samples_leaf': [i for i in range(3, 18, 7)]\n",
    "}\n",
    "\n",
    "grdSrcXgb = GSCV(xgbBaseModel, xgbParams, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "grdSrcXgb.fit(xTrn, yTrn)\n",
    "\n",
    "xgbModelParams = grdSrcXgb.best_params_\n",
    "\n",
    "xgbOptModel = GBR(**xgbModelParams)\n",
    "xgbOptModel.fit(xTrn, yTrn)\n",
    "\n",
    "xgbMseValScores = -1 * CVS(xgbOptModel, xVal, yVal, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "xgbMseTstScores = -1 * CVS(xgbOptModel, xTst, yTst, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "print(f'The mean squared error of the model on the validation set is {xgbMseValScores.mean()}')\n",
    "print(f'The mean squared error of the model on the Test set is {xgbMseTstScores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Submission Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export random forest model\n",
    "yPredRndFrst = np.round(np.exp(rndFrstOptModel.predict(finalTest[features])), decimals = -3)\n",
    "rndFrstSubmission = pd.DataFrame({'id': finalTest.id, 'price_doc': yPredRndFrst})\n",
    "rndFrstSubmission.to_csv('RandomForestSubmission.csv', index = False)\n",
    "\n",
    "# export xgboost model\n",
    "yPredXgb = np.round(np.exp(xgbOptModel.predict(finalTest[[col for col in finalTest.columns if col not in ['id', 'timestamp']]])), decimals = -3)\n",
    "xgbSubmission = pd.DataFrame({'id': finalTest.id, 'price_doc': yPredXgb})\n",
    "xgbSubmission.to_csv('XGBoostSubmission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Baseline Results\n",
    "need to discuss the model's performance metrics\n",
    "\n",
    "therefor need to calculate the model's performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Notebook Analysis\n",
    "## a. Notebook Selection\n",
    "need to choose a notebook\n",
    "\n",
    "Tom recomendation:\n",
    "- combination of 'simple-exploration-notebook-sberbank.ipynb' and 'feature-engineering-validation-strategy.ipynb'\n",
    "- 'basic-time-series-analysis-feature-selection.ipynb'\n",
    "- 'sberbank-russian-housing-market.ipynb' (using PCA, very useful for us i think)\n",
    "- 'a-very-extensive-exploratory-analysis-in-python.ipynb' (havent gone through all of it yet, but seems good in first look)\n",
    "- 'a-very-extensive-sberbank-exploratory-analysis.Rmd' (havent gone through all of it yet, but seems good in first look)\n",
    "\n",
    "## b. Analysis Summary\n",
    "after choosing notebook(s), summarize its methodology, results and conclusions, \n",
    "###### maybe for further use in the project\n",
    "\n",
    "## c. Critique and Improvements\n",
    "give some form of a feedback on how good is the approach of the chosen notebook(s) and what can be improved in it, giving suggestions for improvements and alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Enhancement\n",
    "## a. Feature Engineering\n",
    "we need to perform some feature engineering in a more informed method and explain the rational behind the method and the features\n",
    "\n",
    "maybe examine the correlation of variables and give out a list of low correlated features to make features out of and then use rational to create the features hinted within them, using heatmap or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Enhanced Model Development\n",
    "Explain new found rational of the steps of task 2 as it being redone\n",
    "- data cleaning\n",
    "    - illogical values\n",
    "    - missing values\n",
    "    - highly correlated values (heatmap or something)\n",
    "    - imputation\n",
    "- feature engineering\n",
    "    - using rational explained in previous Sub-Task\n",
    "    - testing specific effectiveness of engineered features using the jointplot function examplified in 'feature-engineering-validation-strategy.ipynb'\n",
    "- Labeling and Rescaling\n",
    "- Spliting data\n",
    "- Model Creation\n",
    "    - Cross validation hyper parameters ranges\n",
    "    - Base Function Parameters\n",
    "    - performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Advanced Modeling\n",
    "## a. Model Implementation\n",
    "Perform Sub-Tasks 3.a and 3.b in code\n",
    "## b. Evaluation and Comparison\n",
    "Compare the results of the models by viewing and discussing their performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from train.csv, test.csv and macro.csv\n",
    "##### For new model influenced by notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SbrBkTrain = pd.read_csv('train.csv', parse_dates=['timestamp']).filter(['full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'kitch_sq', 'state', 'product_type', 'sub_area', 'price_doc', 'timestamp'])\n",
    "SbrBkTest = pd.read_csv('test.csv', parse_dates=['timestamp']).filter(['id', 'full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'kitch_sq', 'state', 'product_type', 'sub_area', 'timestamp'])\n",
    "SbrBkTrain = pd.merge(SbrBkTrain, pd.read_csv('macro.csv', parse_dates=['timestamp']), how = 'left', on='timestamp')\n",
    "SbrBkTest = pd.merge(SbrBkTest, pd.read_csv('macro.csv', parse_dates=['timestamp']), how = 'left', on='timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to make\n",
    "### Handling illogical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to Make\n",
    "### Handling missing values\n",
    "- too much missing values in columns\n",
    "- impute the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### missing values\n",
    "- normalize and impute them? (done in 'basic-time-series-analysis-feature-selection.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for data cleaning\n",
    "##### Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in price_doc\n",
    "ulimit = np.percentile(SbrBkTrain.price_doc.values, 99)\n",
    "llimit = np.percentile(SbrBkTrain.price_doc.values, 1)\n",
    "SbrBkTrain['price_doc'].ix[SbrBkTrain.price_doc>ulimit] = ulimit\n",
    "SbrBkTrain['price_doc'].ix[SbrBkTrain.price_doc<llimit] = llimit\n",
    "\n",
    "# in others, if any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### potential features to make\n",
    "\n",
    "#### timestamp features:\n",
    "- yearNmonth\n",
    "- yearNweek\n",
    "- year\n",
    "- monthOyear\n",
    "- weekOyear\n",
    "- dayOyear\n",
    "- yearNmonthCount- number of houses available in yearNmonth time period: (yearNmonth[i] == yearNmonth).sum() (maybe implement using a loop)\n",
    "- yearNweekCount- number of houses available in yearNweek time period: (yearNweek[i] == yearNweek).sum() (maybe implement using a loop)\n",
    "- lastNdaysCount- number of houses sold in the last n days: using a transformed timestamp to be an int value of the day which is called dayInt, ((dayInt[i] - dayInt <= n) & (dayInt[i] - dayInt >= 0)).sum()\n",
    "\n",
    "#### ratio features:\n",
    "- lifeSqPRfullSq: life_sq[i] / max(full_sq, 1)\n",
    "- lifeSqRfullSq: life_sq[i] / full_sq[i]\n",
    "- kitchSqPRlifeSq: kitch_sq[i] / max(life_sq, 1)\n",
    "- kitchSqRlifeSq: kitch_sq[i] / life_sq[i]\n",
    "- kitchSqPRfullSq: kitch_sq[i] / max(full_sq, 1)\n",
    "- kitchSqRfullSq: kitch_sq[i] / full_sq[i]\n",
    "- floorRmaxFloor: floor[i] / max_floor[i]\n",
    "- preschoolRatio: children_preschool[i] / preschool_quota[i]\n",
    "- schoolRatio: children_school[i] / school_quota[i]\n",
    "\n",
    "#### other features:\n",
    "- number of nulls in the row\n",
    "- number of nulls in the row in accordance to weights related to the number of nulls in the collumn\n",
    "- number of floors from top: max_floor[i] - floor[i]\n",
    "- extraSq: full_sq[i] - life_sq[i] - kitch_sq[i]\n",
    "- buildAge: build_year[i] - year[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in SbrBkTrain.columns:\n",
    "    if SbrBkTrain[col].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(SbrBkTrain[col].values) + list(SbrBkTest[col].values))\n",
    "        SbrBkTrain[col] = lbl.transform(list(SbrBkTrain[col].values))\n",
    "        SbrBkTest[col] = lbl.transform(list(SbrBkTest[col].values))\n",
    "\n",
    "# addition none opject labeling\n",
    "mtrlLbl = LabelEncoder().fit(list(SbrBkTrain.material.values) + list(SbrBkTest.material.values))\n",
    "stateLbl = LabelEncoder().fit(list(SbrBkTrain.state.values) + list(SbrBkTest.state.values))\n",
    "SbrBkTrain.material = mtrlLbl.transform(SbrBkTrain.material.values)\n",
    "SbrBkTest.material = mtrlLbl.transform(SbrBkTest.material.values)\n",
    "SbrBkTrain.state = stateLbl.transform(SbrBkTrain.state.values)\n",
    "SbrBkTest.state = stateLbl.transform(SbrBkTest.state.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### feature selection:\n",
    "- select features by looking at correlation matrix\n",
    "    - out of all variables that are highly correlated with the log(price_doc) choose the most correlated features out of each highly correlated group\n",
    "    - maybe vice versa?\n",
    "    - keep only relevant features\n",
    "    - using feature importance aswell\n",
    "- take method from 'basic-time-series-analysis-feature-selection.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple types of modeling and cross validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. State-Of-The-Art Techniques\n",
    "## a. Technique Exploration\n",
    "further inspection of notebooks from the kaggle competition of the top competitors, searching for advanced techniques to apply in our models to improve them\n",
    "## b. Application and Performance\n",
    "discussing and explaining the works of these techniques, how and why they work.\n",
    "\n",
    "###### maybe even implement them within our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation\n",
    "#### Details\n",
    "5-minute powerpoint presentation that describes the most interesting turning points in the project\n",
    "\n",
    "i think we should include the saving of rows when data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading Criteria\n",
    "- **Clarity and organization:**\n",
    "\n",
    "    the report (presentation and PDF) should have structure and clear in the writing to make it easier for the readers to understand the process and findings\n",
    "- **Comprehensiveness:**\n",
    "\n",
    "    the report (presentation and PDF) should cover all aspects of the tasks having detailed explanations of our approach, analysis and conclusions\n",
    "- **Innovation:**\n",
    "\n",
    "    Creativity and originality is encouraged\n",
    "- **Accuracy and Performance:**\n",
    "\n",
    "    we need to show the improvement of the models when implementing the methodologies and compare the original results with the new ones\n",
    "- **Relative Performance Evaluation:**\n",
    "\n",
    "    the final grade is also assigned based on the performance in the course (sounds like bullshit because they dont have a way to check our performance in the course if attendance is was not a must)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
